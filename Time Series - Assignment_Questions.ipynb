{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment: Time Series Problem.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"G5Q1dSZwEkwd"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"P2QW8PWWHAH_"},"source":["**<center><h3>Time Series Assignment</h3></center>**"]},{"cell_type":"markdown","metadata":{"id":"LSdHJ_8G8FUp"},"source":["---\n","# **Table of Contents**\n","---\n","\n","**1.** [**Problem Statement**](#Section1)<br>\n","**2.** [**Objective**](#Section2)<br>\n","**3.** [**Installing & Importing Libraries**](#Section3)<br>\n","  - **3.1** [**Installing Libraries**](#Section31)\n","  - **3.2** [**Upgrading Libraries**](#Section32)\n","  - **3.3** [**Importing Libraries**](#Section33)\n","\n","**4.** [**Data Acquisition & Description**](#Section4)<br>\n","  - **4.1** [**Data Description**](#Section41)\n","\n","**5.** [**Data Pre-processing**](#Section5)<br>\n","  - **5.1** [**Identification & Handling of Missing Data**](#Section51)<br>\n","  - **5.2** [**Identification & Handling of Redundant Data**](#Section52)<br>\n","  - **5.3** [**Identification & Handling of Inconsistent Data Types**](#Section53)<br>\n","\n","**6.** [**Time Series Analysis**](#Section6)<br>\n","**7.** [**Time Series Forecasting**](#Section7)<br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Fq6f2dwxEy9p"},"source":["---\n","<a name = Section1></a>\n","# **1. Problem Statement**\n","---\n","\n","- In late 2010, Onion prices shot through the roof and cause grave crisis.\n","\n","- This was caused by lack of rainfall in major onion producing region such as Maharashtra and Karnataka.\n","\n","- The crisis has led political tension and large scale hoarding by the traders in the country.\n","\n","- Former Prime Minister Manmohan Singh described it as \"a grave concern\".\n","\n","- **Further Information**:\n","  - BBC Article in Dec 2010 - [**Stink over onion crisis is enough to make you cry**](http://www.bbc.co.uk/blogs/thereporters/soutikbiswas/2010/12/indias_onion_crisis.html)\n","\n","  - Hindu OpEd in Dec 2010 - [**The political price of onions**](http://www.thehindu.com/opinion/editorial/article977100.ece)"]},{"cell_type":"markdown","metadata":{"id":"dR8ryPM5HENa"},"source":["---\n","<a name = Section2></a>\n","# **2. Objective**\n","---\n","\n","- The objective of this assignment is to predict the price of onion in Bangalore using ARIMA."]},{"cell_type":"markdown","metadata":{"id":"pR6Oosm5HPdR"},"source":["---\n","<a name = Section3></a>\n","# **3. Installing & Importing Libraries**\n","---"]},{"cell_type":"markdown","metadata":{"id":"5sCeYVZrHQmS"},"source":["<a name = Section31></a>\n","### **3.1 Installing Libraries**"]},{"cell_type":"code","metadata":{"id":"NTRXv6_THQaq"},"source":["!pip install -q datascience                                         # Package that is required by pandas profiling\n","!pip install -q pandas-profiling                                    # Library to generate basic statistics about data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3S3diVeaHgbp"},"source":["<a name = Section32></a>\n","### **3.2 Upgrading Libraries**\n","\n","- **After upgrading** the libraries, you need to **restart the runtime** to make the libraries in sync. \n","\n","- Make sure not to execute the cell above (3.1) and below (3.2) again after restarting the runtime."]},{"cell_type":"code","metadata":{"id":"vnH4XG7DEY0J"},"source":["!pip install -q --upgrade pandas-profiling\n","!pip install -q --upgrade statsmodels "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABAEnidxHkaq"},"source":["<a name = Section33></a>\n","### **3.3 Importing Libraries**"]},{"cell_type":"code","metadata":{"id":"nPJl-HRFHZ16"},"source":["#-------------------------------------------------------------------------------------------------------------------------------\n","import pandas as pd\n","pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high      \n","pd.set_option('display.max_colwidth', None)                         # Unfolding the max feature width for better clearity      \n","pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high\n","pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations\n","pd.set_option('display.float_format', lambda x: '%.5f' % x)         # To suppress scientific notation over exponential values\n","#-------------------------------------------------------------------------------------------------------------------------------\n","import numpy as np                                                  # Importing package numpys (For Numerical Python)\n","#-------------------------------------------------------------------------------------------------------------------------------\n","from datetime import datetime                                       # Importing datetime for datetime manipulation\n","#-------------------------------------------------------------------------------------------------------------------------------\n","import matplotlib.pyplot as plt                                     # Importing pyplot interface using matplotlib\n","from matplotlib.pylab import rcParams                               # Backend used for rendering and GUI integration                                               \n","import seaborn as sns                                               # Importin seaborm library for interactive visualization\n","%matplotlib inline\n","#-------------------------------------------------------------------------------------------------------------------------------\n","from statsmodels.tsa.seasonal import seasonal_decompose             # Seasonal decomposition using moving averages\n","from statsmodels.tsa.stattools import adfuller                      # Augmented Dickey-Fuller unit root test\n","from statsmodels.tsa.stattools import acf, pacf                     # Importing Autocorrelation and Partial Autocorrelation\n","from statsmodels.graphics.tsaplots import plot_acf                  # To plot Autocorrelation Function\n","from statsmodels.graphics.tsaplots import plot_pacf                 # To plot Partial Autocorrelation Function\n","#-------------------------------------------------------------------------------------------------------------------------------\n","from statsmodels.tsa.arima.model import ARIMA\n","#-------------------------------------------------------------------------------------------------------------------------------\n","import warnings                                                     # Importing warning to disable runtime warnings\n","warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ubvrDy3uHxmw"},"source":["---\n","<a name = Section4></a>\n","# **4. Data Acquisition & Description**\n","---\n","\n","- The data set is based on the amount of onions sold (1996-01, 2016-12) and it can be retrieved from the attached <a href = \"https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/Assignment/MonthWiseMarketArrivals_Clean.csv\">**link**</a>.\n","\n","| Records | Features | Dataset Size |\n","| :-- | :-- | :-- |\n","| 10227 | 10 | 658 KB| \n","\n","|Id|Feature|Description|\n","|:--|:--|:--|\n","|01|**market**|The place where onions are sold.|\n","|02|**month**|Month on which onions were sold.|\n","|03|**year**|Year on which onions were sold.|\n","|04|**quantity**|Quantity of onions sold.|\n","|05|**priceMin**|Minimum prices of Onions.|\n","|06|**priceMax**|Maximum prices of Onions.|\n","|07|**priceMod**|Price Mode of Onions.|\n","|08|**state**|The name of the state where onions were sold.|\n","|09|**city**|The name of the city where onions were sold.|\n","|10|**date**|Date on which onions were sold.|\n"]},{"cell_type":"code","metadata":{"id":"rkct26v0Ibxt"},"source":["data = pd.read_csv('https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/Assignment/MonthWiseMarketArrivals_Clean.csv')\n","print('Data Shape:', data.shape)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xg04KYAbJ7IJ"},"source":["<a name = Section41></a>\n","### **4.1 Data Description**\n","\n","- In this section we will get **information about the data** and see some observations."]},{"cell_type":"code","metadata":{"id":"QYrhbZZvJ_KL"},"source":["print('Described Column Length:', len(data.describe().columns))\n","data.describe().transpose()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvE7jm3rJ6Cw"},"source":["<a name = Section5></a>\n","\n","---\n","# **5. Data Pre-Processing**\n","---"]},{"cell_type":"markdown","metadata":{"id":"pcBLgl7QK-it"},"source":["<a name = Section51></a>\n","### **5.1 Identification & Handling of Missing Data**"]},{"cell_type":"code","metadata":{"id":"qKIII9JeImYy"},"source":["missing_frame = pd.DataFrame(index = data.columns.values)\n","missing_frame['Null Frequency'] = data.isnull().sum().values\n","nullpercent = data.isnull().sum().values/data.shape[0]\n","missing_frame['Missing Null %age'] = np.round(nullpercent, decimals = 4) * 100\n","missing_frame['Zero Frequency'] = data[data == 0].count().values\n","zero_percent = data[data == 0].count().values / data.shape[0]\n","missing_frame['Missing %age'] = np.round(zero_percent, decimals = 4) * 100\n","missing_frame.transpose()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P02evc_FLHe9"},"source":["<a name = Section52></a>\n","### **5.2 Identification & Handling of Redundant Data**\n","\n","- In this section **we will identify redundant rows and columns** in our data if present.\n","\n","- For handling duplicate features we have created a custom function to identify duplicacy in features with different name but similar values:"]},{"cell_type":"code","metadata":{"id":"1rVf47s9LHQ_"},"source":["def duplicate_cols(dataframe):\n","  ls1 = []\n","  ls2 = []\n","\n","  columns = dataframe.columns.values\n","  for i in range(0, len(columns)):\n","    for j in range(i+1, len(columns)):\n","      if (np.where(dataframe[columns[i]] == dataframe[columns[j]], True, False).all() == True):\n","        ls1.append(columns[i])\n","        ls2.append(columns[j])\n","\n","  if ((len(ls1) == 0) & (len(ls2) == 0)):\n","    return None\n","  else:\n","    duplicate_frame = pd.DataFrame()\n","    duplicate_frame['Feature 1'] = ls1\n","    duplicate_frame['Feature 2'] = ls2\n","    return duplicate_frame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bs0j8Te6LLdt"},"source":["print('Contains Redundant Records?:', data.duplicated().any())\n","print('Duplicate Count:', data.duplicated().sum())\n","print('-----------------------------------------------------------------------')\n","print('Contains Redundant Features?:', duplicate_cols(data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxNVJWIZLPji"},"source":["<a name = Section53></a>\n","### **5.3 Identification & Handling of Inconsistent Data Types**\n","\n","- In this section we will **identify** and **handle** the **feature** that may **contains inconsistent data type**."]},{"cell_type":"markdown","metadata":{"id":"c6OGY6g6LTF5"},"source":["**Before Identification & Handling of Inconsistent Data Types**"]},{"cell_type":"code","metadata":{"id":"rjPXQGVrLPTy"},"source":["data.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQQze20xLV2S"},"source":["type_frame = pd.DataFrame(data = data.dtypes, columns = ['Type'])\n","type_frame.transpose()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-zsawkoLkzF"},"source":["**Performing Operations**"]},{"cell_type":"code","metadata":{"id":"v97LjfFTLkqz"},"source":["data['date']  =  pd.to_datetime(data['date'], infer_datetime_format = True) \n","print('Success!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-m4WzvpCMfcR"},"source":["**After Identification & Handling of Inconsistent Data Types**"]},{"cell_type":"code","metadata":{"id":"ukX0uK6UMfSp"},"source":["type_frame = pd.DataFrame(data = data.dtypes, columns = ['Type'])\n","type_frame.transpose()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceaU27VLMphy"},"source":["**Observation:**\n","\n","- We have successfully handeled inconsistent data types."]},{"cell_type":"markdown","metadata":{"id":"tv3lvWe8Mtur"},"source":["<a name = Section6></a>\n","\n","---\n","# **6. Time Series Analysis**\n","---\n","\n","- Time series deals with two columns, i.e. temporal (predictor) and forecast (prediction).\n","\n","  - **Temporal:** The time which in our case is **year**.\n","\n","  - **Forecast:** The price of onions i.e. **priceMod**."]},{"cell_type":"code","metadata":{"id":"9AyMTeOrAhfz"},"source":["def plot_trend():\n","  fig = plt.figure(figsize = [15, 7])\n","  sns.lineplot(x = 'date', y = 'priceMod', data = data, ci = None)\n","  plt.xlabel('Date', size = 14)\n","  plt.ylabel('Price of Onions', size = 14)\n","  plt.title('Trend in Price of Onions (1996 - 2016)', size = 16)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZuMi4FLMapq"},"source":["plot_trend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rp73u15GwYHa"},"source":["- Before going further, let's set date as our index which in result will simplify our work while performing analysis."]},{"cell_type":"code","metadata":{"id":"V_6suJaQvxnl"},"source":["indexed_data =  data.set_index(['date'])\n","print('Success!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbaYDVsJwNGc"},"source":["indexed_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0rML7WK5Arm1"},"source":["---\n","**<h4>Question 1:** Plot the trend and distribution of priceMod by using following instructions:</h4>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"PWXg2rrvBKYJ"},"source":["- Create a new dataframe named as indexed_data_BANGLORE which will contain data related to Banglore city only.\n","\n","- Remove features - market,\tmonth, year, priceMin, priceMax, state, city, quantity.\n","\n","- Sort the dataframe index using sort index function."]},{"cell_type":"code","metadata":{"id":"bQNJ2yEMBG7f"},"source":["def index_frame():\n","  # Insert your code here...\n","  return indexed_data_BANGLORE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qS8oUkLs7pzT"},"source":["indexed_data_BANGLORE = index_frame()\n","indexed_data_BANGLORE.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIIr3EieBT_2"},"source":["- Create a function to plot the trend and distribution of onion prices side by side."]},{"cell_type":"code","metadata":{"id":"q6smYlVVBho-"},"source":["def trend_dist():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmIjzLOuB6SP"},"source":["trend_dist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2ZuDmAaCEGV"},"source":["---\n","**<h4>Question 2:** Generate all seasonal components of time series by performing seasonal decomposition.</h4>\n","\n","---\n","\n","- Use seasonal_decompose() function present in statsmodel over indexed_data_BANGLORE created earlier.\n"]},{"cell_type":"code","metadata":{"id":"9bK92Q0GF13W"},"source":["def seasonalDecompose():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hBK2cQCyCztX"},"source":["seasonalDecompose()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6p0_hcfVC9uk"},"source":["---\n","**<h4>Question 3:** Create a function named as <ins>rolling_means</ins> to estimated Rolling Statistics by using following instructions.\n","\n","---\n","\n","- Calculate rolling mean using rolling window of size 12 and store in a roll_mean variable.\n","\n","- Calculate rolling std using rolling window of size 12 and store in a roll_std variable.\n","\n","- Create line plots for original data (Banglore), rolling mean and rolling standard deviation.\n"]},{"cell_type":"code","metadata":{"id":"O-8yE3EoMOet"},"source":["def rolling_means(data, feature, title_add = ''):\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3ZkocisD_U8"},"source":["---\n","**<h4>Question 4:** Create a function named as <ins>ADFTest</ins> to estimate Augmented Dickeyâ€“Fuller test by using following instructions.\n","\n","---\n","\n","- Calculate an object of adfuller function by passing data column and setting autolag as AIC.\n","\n","- Create a dataframe that explain all the metrics out of the adfuller object.\n","\n","- Return the dataframe object."]},{"cell_type":"code","metadata":{"id":"8-d7LPaKMkwm"},"source":["def ADFTest(data, feature, test_label = 'Original'):\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hhtFgbq2NkGS"},"source":["---\n","**<h4>Question 5:** Check the stationarity of priceMod feature by calling rolling_means() and ADFTest() function.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"fENOqdMMNjpa"},"source":["rolling_means()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMQk8xgZOzyf"},"source":["ADFTest()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xyS0QbtnFemY"},"source":["---\n","\n","**<h4>Question 6:** Perform log transformation over priceMod feature and plot the trend as well as distribution.\n","\n","---\n","\n","- Create a log_indexed_data variable by applying log transformation on indexed_data_BANGLORE dataframe."]},{"cell_type":"code","metadata":{"id":"d6Xs3OpfGHD-"},"source":["def createFeature():\n","  # Insert your code here...\n","  return log_indexed_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvCogEU-mvHP"},"source":["log_indexed_data = createFeature()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSv6Vnh1GTkM"},"source":["- Create a function that displays trend as well the distribution of the onion prices."]},{"cell_type":"code","metadata":{"id":"-nsshiTWO_D8"},"source":["def trend_dist():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qliWGWTHGP6N"},"source":["trend_dist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5VoUP50GvcN"},"source":["---\n","\n","**<h4>Question 7:** Perform Rolling Statistics as well as Augmented Dickey-Fuller Test over the log_indexed_data and priceMod features.\n","\n","---\n","\n","- Create a function that shows side by side comparison of Rolling Statistics of priceMod."]},{"cell_type":"code","metadata":{"id":"ulBHxfNPQyUF"},"source":["def roll_stats():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Va-WeY6JHoCQ"},"source":["roll_stats()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ARb4_H3bH4vv"},"source":["- Call Augmented Dickey Fuller Test function that was created earlier over original indexed_data_BANGLORE dataframe."]},{"cell_type":"code","metadata":{"id":"-R_pOT5BS1On"},"source":["ADFTest()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kpozVw_xIFVv"},"source":["- Call Augmented Dickey Fuller Test function that was created earlier over log transformed indexed_data_BANGLORE dataframe."]},{"cell_type":"code","metadata":{"id":"YQJca7xJTCnX"},"source":["ADFTest()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m95gWRIyIMK9"},"source":["---\n","\n","**<h4>Question 8:** Perform time shift transformation over log_indexed_data dataframe by using following operations.\n","\n","---\n","\n","- Take a difference of log_indexed_data and shifted log_indexed_data by periods = 1 and store inside shift_indexed_data.\n","\n","- Drop all the null values by using dropna function."]},{"cell_type":"code","metadata":{"id":"lnKucgWeJT7Q"},"source":["def shiftTransform():\n","  # Insert your code here...\n","  return shift_indexed_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amjkAjVE8Yrg"},"source":["shift_indexed_data = shiftTransform()\n","shift_indexed_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvxD3Y1NJdnV"},"source":["- Plot a scatterplot between shifted log_indexed_data by periods = 1 and log_indexed_data."]},{"cell_type":"code","metadata":{"id":"yz6mRL8PTRnH"},"source":["def plot_scatter():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wTDiGe5Ja9q"},"source":["plot_scatter()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WvWYgwEsJxLK"},"source":["---\n","\n","**<h4>Question 9:** Perform Rolling Statistics as well as Augmented Dickey-Fuller Test over the shift_indexed_data and priceMod features.\n","\n","---\n","\n","- Create a function that shows side by side comparison of Rolling Statistics of priceMod."]},{"cell_type":"code","metadata":{"id":"R6E1IHNzWKUg"},"source":["def roll_stats():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohwBMcCwKAOP"},"source":["roll_stats()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sd93dxHpKErG"},"source":["- Call Augmented Dickey Fuller Test function that was created earlier over original indexed_data_BANGLORE dataframe."]},{"cell_type":"code","metadata":{"id":"gmaiXKYuWiq1"},"source":["ADFTest()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XxEdIfKzKGmP"},"source":["- Call Augmented Dickey Fuller Test function that was created earlier over shift_indexed_data dataframe."]},{"cell_type":"code","metadata":{"id":"KiMdFBQnWjhM","executionInfo":{"status":"ok","timestamp":1600951744338,"user_tz":-330,"elapsed":2495,"user":{"displayName":"Mukesh Kumar","photoUrl":"","userId":"01238672520445611335"}},"outputId":"029d1bee-b330-4527-991e-3ab0e288c82c","colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["ADFTest()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ADF_Test_Statistics</th>\n","      <th>p-value</th>\n","      <th>Used_Lags</th>\n","      <th>Number_Of_Observations</th>\n","      <th>Critical_Value (1%)</th>\n","      <th>Critical_Value (5%)</th>\n","      <th>Critical_Value (10%)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Shift Transformed</th>\n","      <td>-7.29325</td>\n","      <td>0.00000</td>\n","      <td>7</td>\n","      <td>137</td>\n","      <td>-3.47901</td>\n","      <td>-2.88288</td>\n","      <td>-2.57815</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   ADF_Test_Statistics  p-value  Used_Lags  \\\n","Shift Transformed             -7.29325  0.00000          7   \n","\n","                   Number_Of_Observations  Critical_Value (1%)  \\\n","Shift Transformed                     137             -3.47901   \n","\n","                   Critical_Value (5%)  Critical_Value (10%)  \n","Shift Transformed             -2.88288              -2.57815  "]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"y_j3RAgmjZHK"},"source":["<a name = Section7></a>\n","\n","---\n","# **7. Time Series Forecasting**\n","---"]},{"cell_type":"markdown","metadata":{"id":"fmtkmoZPKdzZ"},"source":["---\n","\n","**<h4>Question 10:** Estimate p and q from autocorrelation and partial autocorrelation function defined in statsmodel.\n","\n","---\n","\n","- Create two variables named as <ins>ACF</ins> and <ins>PACF</ins> to estimate acf() and pacf() using lags = 20.\n","\n","- Create a dataframe named as <ins>corrFrame</ins> using ACF and PACF variables."]},{"cell_type":"code","metadata":{"id":"OCPMt2yVWmIC"},"source":["def autoPartialCorrFrame():\n","  # Estimating Autocorrelation Function\n","  ACF = acf(shift_indexed_data, nlags = 20)\n","\n","  # Estimating Partial Autocorrelation Function\n","  PACF = pacf(shift_indexed_data, nlags = 20)\n","\n","  # Preparing a dataframe out of Correlation Arrays\n","  corrFrame = pd.DataFrame(data = {'ACF': ACF, 'PACF': PACF})\n","\n","  return corrFrame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvnIcagy88ZX"},"source":["corrFrame = autoPartialCorrFrame()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_oavb6hK3Mn"},"source":["- Create two subplots and plot autocorrelation and partial autocorrelation functions simultaneously."]},{"cell_type":"code","metadata":{"id":"4SUnJgNFdcU-"},"source":["def autoCorr_partialAutoCorr():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1epuuAsjLRc3"},"source":["autoCorr_partialAutoCorr()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GlBp4Vj8eiDD"},"source":["- On analyzing above plots we came to conclusion of using p = 2, d = 0 and q = 1."]},{"cell_type":"markdown","metadata":{"id":"W2sdm4iPftLs"},"source":["---\n","\n","**<h4>Question 11:** Create a function named as <ins>actual_vs_predicted</ins> to evaluate ARIMA model later on.\n","\n","---\n","\n","- Plot line plot and scatterplot taking input parameters.\n","\n","- Add a model evaluation metric which will estimate RSS over the data.\n","\n","- Set the RSS over the title to visualize the comparision."]},{"cell_type":"code","metadata":{"id":"Kw_nHWNBfsdf"},"source":["def actual_vs_predicted(actual_data, predicted_data, title):\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRw_YiUcL2Xe"},"source":["---\n","\n","**<h4>Question 12:** Execute ARIMA model with the identified p, d and q value from earlier analysis and evaluate the model.\n","\n","---\n","\n","- Create a function that returns ARIMA and model fit.\n","\n","- Create an object of ARIMA model inside function by passing shift_indexed_data with order = [2, 0, 1]"]},{"cell_type":"code","metadata":{"id":"EzR_cap_dxZH"},"source":["def arimaModel():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X4oy8sbWOk06"},"source":["- Print the model parameters and plot the actual vs predicted values over the data."]},{"cell_type":"code","metadata":{"id":"2ifYq3jXMtXH"},"source":["model, model_fit = arimaModel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhM1Yed4O59G"},"source":["- Evaluate the model by plotting side by side comparison of actual and predicted data."]},{"cell_type":"code","metadata":{"id":"e0bU0OV1ObQw"},"source":["def evalModel():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TpiLOKbPOU-"},"source":["evalModel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vLUhkFymMzcW"},"source":["---\n","\n","**<h4>Question 13:** Perform the reverse transformation over the estimated values from the model.\n","\n","---\n","\n","- Create a dataframe named as <ins>reverse_diff_data</ins> having first value of original column i.e. priceMod.\n","\n","- Add the cummulative sum over model_fit fitted values with the dataframe named as <ins>reverse_diff_data</ins>.\n","\n","- Then assign back the first value of original feature to the first value of reverse_diff_data.\n","\n","- Finally return reverse_diff_data dataframe."]},{"cell_type":"code","metadata":{"id":"96tDNl7s5KHp"},"source":["def reverseTransform():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fN0vsaAPQSFH"},"source":["- Call reverse_diff_data in a reverse_diff_data variable and plot first five values using .head()"]},{"cell_type":"code","metadata":{"id":"tcW--Sw9PU0A"},"source":["reverse_diff_data = reverseTransform()\n","reverse_diff_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYfSCe1Zlg0P"},"source":["---\n","\n","**<h4>Question 14:** Perform exponential transformation over reverse_diff_data and view first five data points.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"MqiF_cp_85AB"},"source":["def inverseShift():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0X85Q-QPoeu"},"source":["inverse_log_data = inverseShift()\n","inverse_log_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYuvr_4gQl9K"},"source":["\n","- Plot the actual values and predicted values to original scale."]},{"cell_type":"code","metadata":{"id":"MvE4YgnllU8A"},"source":["def plotReverseTransData():\n","  # Insert your code here..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NpUT3L2pP-1k"},"source":["plotReverseTransData()"],"execution_count":null,"outputs":[]}]}